{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_DICT = {\n",
        "    'configuration': 'LF',\n",
        "    'path_train': 'data/Train',\n",
        "    'path_test': 'data/Test',\n",
        "    'win_size': 100\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WswEsEiCEiUJ",
        "outputId": "b21ebc1c-5e7f-4344-b998-7ff763aeb14f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, brier_score_loss\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from scipy.stats import kurtosis, skew, trim_mean, iqr\n",
        "\n",
        "#from sktime.classification.feature_based import Catch22Classifier\n",
        "from sktime.datatypes._panel._convert import from_3d_numpy_to_nested\n",
        "\n",
        "#from tslearn.early_classification import NonMyopicEarlyClassifier\n",
        "#from tslearn.utils import to_time_series_dataset, to_time_series\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iA6oIERzEiUN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "rcParams['figure.figsize'] = 16, 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert os.listdir(CONFIG_DICT['path_train']) is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat(\n",
        "    [pd.read_csv(os.path.join(CONFIG_DICT['path_train'], p)) for p in os.listdir(CONFIG_DICT['path_train'])]\n",
        ")\n",
        "df_test_temp = pd.concat(\n",
        "    [pd.read_csv(os.path.join(CONFIG_DICT['path_test'], p)) for p in os.listdir(CONFIG_DICT['path_test'])]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQxIG1EnuRB",
        "outputId": "c11479ba-81f0-4d50-80be-7dc39a05c5da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Indoor', 'MagLF_Norm', 'MagLF_x', 'MagLF_y', 'MagLF_z']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "configuration = CONFIG_DICT['configuration']\n",
        "\n",
        "for c in df.columns:\n",
        "    if configuration:\n",
        "        if not (c.startswith(f'Mag{configuration}') or c.startswith('Indoor') or c =='Patient' or c.startswith('T') or c=='Date'):\n",
        "            df.drop(columns=c, inplace=True)\n",
        "            df_test_temp.drop(columns=c, inplace=True)\n",
        "    else:\n",
        "        if not (c.startswith('Mag') or c.startswith('Indoor') or c=='Patient' or c.startswith('T') or c=='Date'):\n",
        "            df.drop(columns=c, inplace=True)\n",
        "            df_test_temp.drop(columns=c, inplace=True)\n",
        "\n",
        "for col in df.columns:\n",
        "  if df[col].dtypes=='float64':\n",
        "    df[col] = df[col].astype('float32')\n",
        "    df_test_temp[col] = df_test_temp[col].astype('float32')\n",
        "  if df[col].dtype=='int64':\n",
        "    df[col] = df[col].astype('int16')\n",
        "    df_test_temp[col] = df_test_temp[col].astype('int16')\n",
        "\n",
        "df['Timestamp'] = pd.to_datetime(df['Timestamp'], unit='s')\n",
        "df_test_temp['Timestamp'] = pd.to_datetime(df_test_temp['Timestamp'], unit='s')\n",
        "\n",
        "#sorted(df_test_temp['Patient'].unique())\n",
        "not_cols = set(['Timestamp', 'Date', 'Patient'])\n",
        "allCols = set(df.columns.to_list())\n",
        "\n",
        "FEATURE_COLS = sorted(list(set.difference(allCols, not_cols)))\n",
        "FEATURE_COLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bulH_-4fEiUT"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(columns=FEATURE_COLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOacIqVSEiUU",
        "outputId": "c0d9b3eb-f098-4b07-aa90-ccf8f9128440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing patient:  1004\n",
            "Processing patient:  3000\n"
          ]
        }
      ],
      "source": [
        "for k, group in df.groupby('Patient'):\n",
        "  print('Processing patient: ', k)\n",
        "  #print(type(group))\n",
        "  if k not in [6001, 6002, 6003]:\n",
        "    for ts in group.Timestamp.unique():\n",
        "      if group[group['Timestamp']==ts]['Timestamp'].value_counts().values in [12800, 12799, 12801]:\n",
        "        data = group[group['Timestamp']==ts][FEATURE_COLS]\n",
        "        df_train = pd.concat([df_train, data])\n",
        "  else:\n",
        "    data = group[~(group['Date'].isin(['13/04/2022 10:05:37','13/04/2022 18:34:28','14/06/2022 11:16:13',\n",
        "                                      '14/06/2022 14:20:03', '14/04/2022 07:58:00', '14/04/2022 16:22:01']))][FEATURE_COLS]\n",
        "    df_train = df_train.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1gX7V0UVEiUU"
      },
      "outputs": [],
      "source": [
        "df_train['series_id'] = np.arange(len(df_train)) // CONFIG_DICT['win_size'] + 1\n",
        "y_train = df_train[['series_id', 'Indoor']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W61O43-zM40x",
        "outputId": "eaf221d4-7e56-46c8-f160-9d9bb89143d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6451544088909752"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.Indoor.sum()/len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YJ1eTOhVEiUV"
      },
      "outputs": [],
      "source": [
        "count_train, count_test, count_val = 0, 0, 0\n",
        "\n",
        "for t in df_train.series_id.value_counts().values:\n",
        "  if t!=CONFIG_DICT['win_size']:\n",
        "    count_train+=t\n",
        "\n",
        "if count_train!=0:\n",
        "  df_train = df_train.iloc[:-count_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Qw8abRbFEiUV"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scl = StandardScaler()\n",
        "\n",
        "#df_train[FEATURE_COLS[1:]] = scl.fit_transform(df_train[FEATURE_COLS[1:]])\n",
        "y_train = df_train[['series_id', 'Indoor']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkS8w2EwEiUV",
        "outputId": "3209599c-5f3c-4e46-c59a-3cf4022191c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18432/18432 [00:54<00:00, 340.47it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(18432, 4, 100)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_sequences = []\n",
        "labels = []\n",
        "for series_id, group in tqdm(df_train.groupby('series_id'), position=0, leave=True):\n",
        "  sequence_features = group[FEATURE_COLS[1:]].to_numpy()\n",
        "  labels.append(y_train[y_train.series_id==series_id].iloc[0].Indoor)\n",
        "  train_sequences.append(sequence_features)\n",
        "\n",
        "train_sequences = np.array(train_sequences)\n",
        "train_sequences = np.swapaxes(train_sequences, 1,2)\n",
        "train_sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wkfiz_OtEiUV"
      },
      "outputs": [],
      "source": [
        "train_df = from_3d_numpy_to_nested(train_sequences, column_names=FEATURE_COLS[1:], cells_as_numpy=True)\n",
        "y_train = pd.DataFrame(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1f6YTwsEiUW"
      },
      "source": [
        "VANILLA STATISTICS\n",
        "\n",
        "* mean,\n",
        "* median,\n",
        "* std dev,\n",
        "* mean absolute deviation\n",
        "* percentiles (1,10,25,50,75,99)\n",
        "* iqr\n",
        "* trimmed mean (0.125)\n",
        "\n",
        "\n",
        "TIME DOMAIN\n",
        "\n",
        "* root mean square,\n",
        "* variance,\n",
        "* kurtosis,\n",
        "* skewness\n",
        "* correlation between each pair of magnetometer\n",
        "\n",
        "FREQUENCY DOMAIN\n",
        "\n",
        "* the dominant frequency\n",
        "* the power at the dominant frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sGycyQKwEiUW"
      },
      "outputs": [],
      "source": [
        "def calc_corr(data,axis,sensor):\n",
        "    if axis=='xy':\n",
        "        corr_xy = np.corrcoef(data[f'Mag{sensor}_{axis[0]}'], data[f'Mag{sensor}_{axis[1]}'])\n",
        "        return corr_xy[0][1]\n",
        "    elif axis=='xz':\n",
        "        corr_xz = np.corrcoef(data[f'Mag{sensor}_{axis[0]}'], data[f'Mag{sensor}_{axis[1]}'])\n",
        "        return corr_xz[0][1]\n",
        "    elif axis=='yz':\n",
        "        corr_yz = np.corrcoef(data[f'Mag{sensor}_{axis[0]}'], data[f'Mag{sensor}_{axis[1]}'])\n",
        "        return corr_yz[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hLZSend2EiUW"
      },
      "outputs": [],
      "source": [
        "def extract_dominant(data):\n",
        "    time_step = 1/100\n",
        "\n",
        "    Hn = np.fft.fft(data)\n",
        "    freqs = np.fft.fftfreq(len(Hn), time_step)\n",
        "    idx = np.argsort(np.abs(Hn))\n",
        "\n",
        "    fs = freqs[idx]\n",
        "    return (fs[0], fs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tP0rHO6NM40z"
      },
      "outputs": [],
      "source": [
        "def extract_power(data):\n",
        "    ps = np.abs(np.fft.fft(data))**2\n",
        "    time_step = 1/100\n",
        "\n",
        "    freqs = np.fft.fftfreq(len(data), time_step)\n",
        "    idx = np.argsort(freqs)\n",
        "    ps = ps[idx]\n",
        "\n",
        "    return (ps[0], ps[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WOlFsiubEiUW"
      },
      "outputs": [],
      "source": [
        "def transform_dataset(df):\n",
        "    sensors = ['LB', 'LF', 'RF', 'WR'] if not configuration else [configuration]\n",
        "    print(sensors)\n",
        "\n",
        "    time_feat_1 =  {\n",
        "        'mean': np.array([np.array([np.mean(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'median': np.array([np.array([np.median(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'std': np.array([np.array([np.std(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'mad': np.array([np.array([np.mean(np.abs(row[c]-np.mean(row[c]))) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'rms': np.array([np.array([np.sqrt(np.mean(row[c]**2)) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'vars': np.array([np.array([np.var(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'kurts': np.array([np.array([kurtosis(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'skews': np.array([np.array([skew(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'p_1': np.array([np.array([np.percentile(row[c], 1) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'p_10': np.array([np.array([np.percentile(row[c], 10) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'p_25': np.array([np.array([np.percentile(row[c], 25) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'p_50': np.array([np.array([np.percentile(row[c], 50) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'p_75': np.array([np.array([np.percentile(row[c], 75) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'p_99': np.array([np.array([np.percentile(row[c], 99) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'iqr': np.array([np.array([iqr(row[c]) for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'trim_mean125': np.array([np.array([trim_mean(row[c], 0.125) for c in row.keys()]) for _,row in df.iterrows()])\n",
        "    }\n",
        "\n",
        "    time_feat_2 = {\n",
        "        'c_xy': np.array([np.array([calc_corr(row, 'xy', s) for s in sensors]) for _,row in df.iterrows()]),\n",
        "        'c_xz': np.array([np.array([calc_corr(row, 'xz', s) for s in sensors]) for _,row in df.iterrows()]),\n",
        "        'c_yz': np.array([np.array([calc_corr(row, 'yz', s) for s in sensors]) for _,row in df.iterrows()])\n",
        "    }\n",
        "\n",
        "    freq_feat = {\n",
        "        'f0': np.array([np.array([extract_dominant(row[c])[0] for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'f1': np.array([np.array([extract_dominant(row[c])[1] for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'pwr0': np.array([np.array([extract_power(row[c])[0] for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "        'pwr1': np.array([np.array([extract_power(row[c])[1] for c in row.keys()]) for _,row in df.iterrows()]),\n",
        "\n",
        "    }\n",
        "\n",
        "    time_df_1 = pd.concat([pd.DataFrame(time_feat_1[tf],\n",
        "        columns=[f'{tf}_dim{i}' for i in range(4*len(sensors))]) for tf in tqdm(time_feat_1.keys())], axis=1)\n",
        "    time_df_2 = pd.concat([pd.DataFrame(time_feat_2[tf],\n",
        "        columns=[f'{tf}_dim{i}' for i in range(len(sensors))]) for tf in tqdm(time_feat_2.keys())], axis=1)\n",
        "    freq_df = pd.concat([pd.DataFrame(freq_feat[ff],\n",
        "        columns=[f'{ff}_dim{i}' for i in range(4*len(sensors))]) for ff in tqdm(freq_feat.keys())], axis=1)\n",
        "\n",
        "    X = pd.concat([time_df_1, time_df_2, freq_df], axis=1)\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi6xSYgVEiUX",
        "outputId": "856ce50c-c954-4112-f8cb-8dd64ba5cbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['LF']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 8004.40it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 2969.77it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 4033.95it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train = transform_dataset(train_df)\n",
        "X_train_scl = scl.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HljXNwvYEiUX",
        "outputId": "5846ae75-0750-46cc-8ff9-979ff010f8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13 features explain around 85.0% of the variance.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit_transform(X_train_scl)\n",
        "var_wanted = 0.85\n",
        "\n",
        "total = sum(pca.explained_variance_)\n",
        "k_pca = 0\n",
        "current_variance = 0\n",
        "while current_variance/total < var_wanted:\n",
        "    current_variance += pca.explained_variance_[k_pca]\n",
        "    k_pca = k_pca + 1\n",
        "\n",
        "print(k_pca, f\"features explain around {var_wanted*100}% of the variance.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qq3jk-p0M40z"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=k_pca)\n",
        "X_train_pca = pca.fit_transform(X_train_scl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jEmvEErdM400"
      },
      "outputs": [],
      "source": [
        "def plot_auroc(y_test, y_pred_proba, show=False):\n",
        "    if (y_test.squeeze().sum()/y_test.shape[0]<=0.05) or (y_test.squeeze().sum()/y_test.shape[0]>=0.95):\n",
        "        print('Test set too unbalanced. No AUROC is provided')\n",
        "    else:\n",
        "        auc = roc_auc_score(y_test.astype(dtype=np.int8), y_pred_proba)\n",
        "        print('AUROC Score: ', auc)\n",
        "        if show:\n",
        "            plt.figure(figsize=(8,5))\n",
        "            fpr, tpr, _ = roc_curve(y_test.astype(dtype=np.int8), y_pred_proba)\n",
        "            plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "            plt.legend(loc=4)\n",
        "            #plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "yU4PzZKpEiUX"
      },
      "outputs": [],
      "source": [
        "def clf_report(clf, X_test, y_test):\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
        "    print(f'Brier Score: {brier_score_loss(y_test, y_pred_proba)}')\n",
        "    plot_auroc(y_test, y_pred_proba)\n",
        "    print('Classification report: ')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    #with open(file_to_write, 'a') as f:\n",
        "    #  f.write(f'Brier Score: {brier_score_loss(y_test, y_pred_proba)}')\n",
        "    #  f.write('\\n')\n",
        "    #  f.write(f'AUROC Score: {auc}')\n",
        "    #  f.write('\\n')\n",
        "    #  f.write(classification_report)\n",
        "\n",
        "    print('-'*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qpc1dj93M400"
      },
      "outputs": [],
      "source": [
        "isTest = False\n",
        "\n",
        "def fit_models(X, X_scaled, y, fit=True):\n",
        "    if not isTest:\n",
        "        ''' Random Forest '''\n",
        "        rfc = RandomForestClassifier(\n",
        "            n_jobs=-1,\n",
        "            random_state=42,\n",
        "            n_estimators=200,\n",
        "            max_features=0.5,\n",
        "            max_depth=4\n",
        "        )\n",
        "\n",
        "        ''' XGB '''\n",
        "        bst = xgb.XGBClassifier(\n",
        "                subsample = 0.5,\n",
        "                min_child_weight = 1,\n",
        "                max_depth=2,\n",
        "                objective = 'binary:logistic'\n",
        "        )\n",
        "        \n",
        "    clfs = [\n",
        "        (rfc, 'RF'),\n",
        "        (bst, 'XGB')\n",
        "\n",
        "    ]\n",
        "\n",
        "    if fit:\n",
        "        for c in clfs:\n",
        "            if c[1]=='XGB':\n",
        "                c[0].fit(X_scaled, y)\n",
        "            else:\n",
        "                c[0].fit(X, y)\n",
        "\n",
        "    return clfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = y_train.astype('int8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "rMjgMUHbM400",
        "outputId": "2cdb4800-1837-4416-e462-8d84c1413856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[02:05:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
            "[02:05:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ]
        }
      ],
      "source": [
        "clf = fit_models(X_train, X_train_scl, y_train)\n",
        "clf_pca = fit_models(X_train_pca, X_train_pca, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVtpECL-M401",
        "outputId": "92e16b32-0547-4127-f72d-bcb0ac60d39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing patient:  2007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9856/9856 [00:17<00:00, 563.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9856, 4, 100)\n",
            "(9856, 4, 100)\n",
            "['LF']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 5332.87it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 2891.29it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 4001.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performances for subject: 2007\n",
            "\n",
            "RF\n",
            "Brier Score: 0.07893348708949222\n",
            "Test set too unbalanced. No AUROC is provided\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       1.00      0.95      0.97      9856\n",
            "\n",
            "    accuracy                           0.95      9856\n",
            "   macro avg       0.50      0.47      0.49      9856\n",
            "weighted avg       1.00      0.95      0.97      9856\n",
            "\n",
            "--------------------\n",
            "Brier Score: 0.05047747043976902\n",
            "Test set too unbalanced. No AUROC is provided\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       1.00      0.95      0.98      9856\n",
            "\n",
            "    accuracy                           0.95      9856\n",
            "   macro avg       0.50      0.48      0.49      9856\n",
            "weighted avg       1.00      0.95      0.98      9856\n",
            "\n",
            "--------------------\n",
            "XGB\n",
            "Brier Score: 0.04719168879699655\n",
            "Test set too unbalanced. No AUROC is provided\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       1.00      0.96      0.98      9856\n",
            "\n",
            "    accuracy                           0.96      9856\n",
            "   macro avg       0.50      0.48      0.49      9856\n",
            "weighted avg       1.00      0.96      0.98      9856\n",
            "\n",
            "--------------------\n",
            "Brier Score: 0.0595632128480375\n",
            "Test set too unbalanced. No AUROC is provided\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       1.00      0.93      0.97      9856\n",
            "\n",
            "    accuracy                           0.93      9856\n",
            "   macro avg       0.50      0.47      0.48      9856\n",
            "weighted avg       1.00      0.93      0.97      9856\n",
            "\n",
            "--------------------\n",
            "Logistic\n",
            "Brier Score: 0.023784012495832636\n",
            "Test set too unbalanced. No AUROC is provided\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       1.00      0.98      0.99      9856\n",
            "\n",
            "    accuracy                           0.98      9856\n",
            "   macro avg       0.50      0.49      0.49      9856\n",
            "weighted avg       1.00      0.98      0.99      9856\n",
            "\n",
            "--------------------\n",
            "Brier Score: 0.0320511534942519\n",
            "Test set too unbalanced. No AUROC is provided\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       1.00      0.97      0.99      9856\n",
            "\n",
            "    accuracy                           0.97      9856\n",
            "   macro avg       0.50      0.49      0.49      9856\n",
            "weighted avg       1.00      0.97      0.99      9856\n",
            "\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for p_test in df_test_temp.Patient.unique():\n",
        "\n",
        "  df_test = pd.DataFrame(columns=FEATURE_COLS)\n",
        "\n",
        "  for k, group in df_test_temp[df_test_temp['Patient']==p_test].groupby('Patient'):\n",
        "    print('Processing patient: ', k)\n",
        "    #print(type(group))\n",
        "    if k not in [6001, 6002, 6003]:\n",
        "      for ts in group.Timestamp.unique():\n",
        "        if group[group['Timestamp']==ts]['Timestamp'].value_counts().values in [12800, 12799, 12801]:\n",
        "          data = group[group['Timestamp']==ts][FEATURE_COLS]\n",
        "          df_test = pd.concat([df_test, data])\n",
        "    else:\n",
        "      data = group[~(group['Date'].isin(['13/04/2022 10:05:37','13/04/2022 18:34:28','14/06/2022 11:16:13',\n",
        "                                        '14/06/2022 14:20:03', '14/04/2022 07:58:00', '14/04/2022 16:22:01']))][FEATURE_COLS]\n",
        "      df_test = pd.concat([df_test, data])\n",
        "\n",
        "  df_test['series_id'] = np.arange(len(df_test)) // CONFIG_DICT['win_size'] + 1\n",
        "  y_test = df_test[['series_id', 'Indoor']]\n",
        "  #y_test = df_test['Indoor']\n",
        "  count_test = 0\n",
        "\n",
        "  for t in df_test.series_id.value_counts().values:\n",
        "    if t!=CONFIG_DICT['win_size']:\n",
        "      count_test+=t\n",
        "\n",
        "  if count_test!=0:\n",
        "    df_test = df_test.iloc[:-count_test]\n",
        "\n",
        "  #if to_scl:\n",
        "  #df_test[FEATURE_COLS[1:]] = scl.transform(df_test[FEATURE_COLS[1:]])\n",
        "  y_test = df_test[['series_id', 'Indoor']]\n",
        "\n",
        "  test_sequences = []\n",
        "  labels_test = []\n",
        "\n",
        "  for series_id, group in tqdm(df_test.groupby('series_id'), position=0, leave=True):\n",
        "    sequence_features = group[FEATURE_COLS[1:]].to_numpy()\n",
        "    labels_test.append(y_test[y_test.series_id==series_id].iloc[0].Indoor)\n",
        "    test_sequences.append(sequence_features)\n",
        "\n",
        "  test_sequences = np.array(test_sequences)\n",
        "  test_sequences = np.swapaxes(test_sequences,1,2)\n",
        "\n",
        "  print(test_sequences.shape)\n",
        "\n",
        "  test_df = from_3d_numpy_to_nested(test_sequences, column_names=FEATURE_COLS[1:], cells_as_numpy=True)\n",
        "  y_test = pd.DataFrame(labels_test)\n",
        "  print(test_sequences.shape)\n",
        "  X_test = transform_dataset(test_df)\n",
        "  X_test_scl = scl.transform(X_test)\n",
        "  print(f'Performances for subject: {p_test}\\n')\n",
        "\n",
        "  X_test_pca = pca.transform(X_test_scl)\n",
        "\n",
        "  var_exp = pca.explained_variance_ratio_.cumsum()\n",
        "  var_exp = var_exp*100\n",
        "  #plt.bar(range(k), var_exp)\n",
        "\n",
        "  for c,c_pca in zip(clf, clf_pca):\n",
        "      print(c[1])\n",
        "      if c[1]=='XGB':\n",
        "        clf_report(c[0], X_test_scl, y_test)\n",
        "      else:\n",
        "        clf_report(c[0], X_test, y_test)\n",
        "      clf_report(c_pca[0], X_test_pca, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "9324f6f91069ef608944cf59327718832b88647e83e66beddcee769fe0e7a057"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
