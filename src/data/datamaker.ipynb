{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFHS71wLVQiN"
      },
      "source": [
        "# Notebook to generate table-based data for model injection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0CRuF12gKG6I"
      },
      "outputs": [],
      "source": [
        "#Uncomment the next two lines if your are using Google Colab\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Etm_aw84b_"
      },
      "outputs": [],
      "source": [
        "data_folder = '/content/drive/Shareddrives/Shared_Stefano_Vicenzo/Acquisitions/Data/TVS'\n",
        "data_format = 'data.mat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRFvqj22pFof"
      },
      "outputs": [],
      "source": [
        "!pip install matgrab --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgWApyS64NJn"
      },
      "outputs": [],
      "source": [
        "!pip install sktime --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTQoy2tBJlSK"
      },
      "outputs": [],
      "source": [
        "import scipy.io as spio\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_l_UYAhJ2Nq"
      },
      "outputs": [],
      "source": [
        "def loadmat(filename):\n",
        "    '''\n",
        "    this function should be called instead of direct spio.loadmat\n",
        "    as it cures the problem of not properly recovering python dictionaries\n",
        "    from mat files. It calls the function check keys to cure all entries\n",
        "    which are still mat-objects\n",
        "    '''\n",
        "    #print(filename)\n",
        "    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
        "    return _check_keys(data)\n",
        "\n",
        "def _check_keys(dict):\n",
        "    '''\n",
        "    checks if entries in dictionary are mat-objects. If yes\n",
        "    todict is called to change them to nested dictionaries\n",
        "    '''\n",
        "    for key in dict:\n",
        "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
        "            dict[key] = _todict(dict[key])\n",
        "    return dict\n",
        "\n",
        "def _todict(matobj):\n",
        "    '''\n",
        "    A recursive function which constructs from matobjects nested dictionaries\n",
        "    '''\n",
        "    dict = {}\n",
        "    for strg in matobj._fieldnames:\n",
        "        elem = matobj.__dict__[strg]\n",
        "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
        "            dict[strg] = _todict(elem)\n",
        "        else:\n",
        "            dict[strg] = elem\n",
        "    return dict\n",
        "\n",
        "def print_mat_nested(d, indent=0, nkeys=0):\n",
        "    # Subset dictionary to limit keys to print.  Only works on first level\n",
        "    if nkeys>0:\n",
        "        d = {k: d[k] for k in list(d.keys())[:nkeys]}  # Dictionary comprehension: limit to first nkeys keys.\n",
        "\n",
        "    if isinstance(d, dict):\n",
        "        for key, value in d.items():         # iteritems loops through key, value pairs\n",
        "          print('\\t' * indent + 'Key: ' + str(key))\n",
        "          print_mat_nested(value, indent+1)\n",
        "\n",
        "    if isinstance(d,np.ndarray) and d.dtype.names is not None:  # Note: and short-circuits by default\n",
        "        for n in d.dtype.names:    # This means it's a struct, it's bit of a kludge test.\n",
        "            print('\\t' * indent + 'Field: ' + str(n))\n",
        "            print_mat_nested(d[n], indent+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Om97hiLSHzM"
      },
      "outputs": [],
      "source": [
        "def filter_values(context_file, df, num_ts):\n",
        "  context = context_file['contextValues']\n",
        "  start = 0\n",
        "  end = 0\n",
        "  filtered_ctx = []\n",
        "  for i,t in enumerate(context.keys()):\n",
        "    if int(t)>=math.trunc(df['Timestamp (ms)'].iloc[0]) and int(t)<=math.trunc(df['Timestamp (ms)'].iloc[-1]):\n",
        "      filtered_ctx.append([int(t), context[t][0], context[t][1], context[t][2], context[t][3]])\n",
        "\n",
        "  filtered_ctx = list(itertools.chain.from_iterable(itertools.repeat(x, 100) for x in filtered_ctx))[:num_ts]\n",
        "  return filtered_ctx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7bBTMlv9koF"
      },
      "outputs": [],
      "source": [
        "def checkIndoor(filtered_ctx, staypts):\n",
        "  indoor=[]\n",
        "  for j,el in enumerate(filtered_ctx):\n",
        "    #stay_id.append(filtered_ctx[j][2])\n",
        "    if filtered_ctx[j][1]!=50:\n",
        "      if filtered_ctx[j][1]>50:\n",
        "        indoor.append(1)\n",
        "      else:\n",
        "        indoor.append(0)\n",
        "    else:\n",
        "      indoor.append(0.5)\n",
        "  return indoor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgoUORXCiqeQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_subject(df):\n",
        "  for c in df.columns:\n",
        "    if not ('Mag' in c or c.startswith('T') or c=='Indoor Probability' or c=='Patient ID'):\n",
        "      df.drop(columns=c, inplace=True)\n",
        "\n",
        "  col_names = {'LowerBack': 'LB', 'LeftFoot': 'LF', 'RightFoot':'RF', 'Wrist':'WR'}\n",
        "\n",
        "  for sensor in col_names.keys():\n",
        "    sens_cols = [col for col in df.columns if sensor in col]\n",
        "    df[f'Mag{sensor}_Norm'] = np.linalg.norm(df[sens_cols].values,axis=1)\n",
        "\n",
        "  for c in df.columns:\n",
        "    for k in col_names.keys():\n",
        "      if k in c:\n",
        "        if 'Norm' in c:\n",
        "          ax = c.split('_')[-1]\n",
        "        else:\n",
        "          ax = c.split('_')[-1].lower()[0]\n",
        "        df.rename(columns={c:f'Mag{col_names[k]}_{ax}'}, inplace=True)\n",
        "\n",
        "    if 'Timestamp' in c:\n",
        "      df.rename(columns={c:'Timestamp'}, inplace=True)\n",
        "    elif 'Indoor' in c:\n",
        "      df.rename(columns={c:'Indoor'}, inplace=True)\n",
        "    elif 'Patient' in c:\n",
        "      df.rename(columns={c:'Patient'}, inplace=True)\n",
        "\n",
        "  df.reset_index(inplace=True, drop=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKvMkYy9yNYr"
      },
      "outputs": [],
      "source": [
        "def makeData(data_folder, data_format, ts=997417, verbose=False):\n",
        "  #i=0\n",
        "  full_df = pd.DataFrame()\n",
        "  for folder in os.listdir(data_folder):\n",
        "    for i,patient in enumerate(os.listdir(os.path.join(data_folder, folder))):\n",
        "      try:\n",
        "        print(f\"Processing patient id: {patient}\")\n",
        "        if os.path.isfile(f'/content/drive/Shareddrives/Shared_Stefano_Vicenzo/Code/Data/df_{patient}.csv'):\n",
        "          print(\"Existing file for ID: \", patient)\n",
        "\n",
        "        else:\n",
        "          #-------------LOAD DATA-------------\n",
        "          filemat = os.path.join(data_folder, folder, patient, 'Out of Lab', data_format )\n",
        "          matdata = loadmat(filemat)\n",
        "          recording = matdata['data']['TimeMeasure1']['Recording4']\n",
        "          if verbose:\n",
        "            print_mat_nested(recording)\n",
        "\n",
        "          #---------CREATE RAW DATASET--------\n",
        "          df = pd.json_normalize(recording)\n",
        "\n",
        "          df_triaxial = pd.DataFrame()\n",
        "          for c in df.columns:\n",
        "            if 'Acc' in c and 'Fs' not in c:\n",
        "              df_triaxial[f'{c}_X (g)'] = pd.Series([v[0] for v in df[c].values[0]])\n",
        "              df_triaxial[f'{c}_Y (g)'] = pd.Series([v[1] for v in df[c].values[0]])\n",
        "              df_triaxial[f'{c}_Z (g)'] = pd.Series([v[2] for v in df[c].values[0]])\n",
        "            elif 'Gyr' in c and 'Fs' not in c:\n",
        "              df_triaxial[f'{c}_X (deg/s)'] = pd.Series([v[0] for v in df[c].values[0]])\n",
        "              df_triaxial[f'{c}_Y (deg/s)'] = pd.Series([v[1] for v in df[c].values[0]])\n",
        "              df_triaxial[f'{c}_Z (deg/s)'] = pd.Series([v[2] for v in df[c].values[0]])\n",
        "            elif 'Mag' in c and 'Fs' not in c:\n",
        "              df_triaxial[f'{c}_X (uT)'] = pd.Series([v[0] for v in df[c].values[0]])\n",
        "              df_triaxial[f'{c}_Y (uT)'] = pd.Series([v[1] for v in df[c].values[0]])\n",
        "              df_triaxial[f'{c}_Z (uT)'] = pd.Series([v[2] for v in df[c].values[0]])\n",
        "\n",
        "          df_triaxial['Timestamp (ms)'] = recording['SU_INDIP']['LowerBack']['Timestamp']\n",
        "          #---------FILTER RAW DATASET--------\n",
        "          indoor = []\n",
        "          stay_id = []\n",
        "          for ctxs in os.listdir(os.path.join(data_folder, folder, patient,\n",
        "                                              'Out of Lab/Contextual Factors')):\n",
        "            if ctxs.startswith('stay'):\n",
        "              staypts = pd.read_json(os.path.join(data_folder, folder, patient,\n",
        "                                    'Out of Lab/Contextual Factors', ctxs))['data']\n",
        "            elif ctxs.startswith('per'):\n",
        "              ctx = pd.read_json(os.path.join(data_folder, folder, patient,\n",
        "                                    'Out of Lab/Contextual Factors', ctxs))['data'][0]\n",
        "            elif ctxs.startswith('path'):\n",
        "              path = pd.read_json(os.path.join(data_folder, folder, patient,\n",
        "                                    'Out of Lab/Contextual Factors', ctxs))\n",
        "\n",
        "          filtered_ctx = filter_values(ctx, df_triaxial, ts)\n",
        "          df_triaxial['Indoor Probability'] = pd.Series(checkIndoor(filtered_ctx, staypts))\n",
        "\n",
        "\n",
        "          df_triaxial['Patient ID'] = pd.Series([patient for _ in range(len(df_triaxial))])\n",
        "          df_triaxial['Disease'] = pd.Series([folder for _ in range(len(df_triaxial))])\n",
        "          print(len(df_triaxial))\n",
        "          #df = pd.DataFrame(df)\n",
        "          #df_triaxial.set_index('Timestamp (ms)', inplace=True)\n",
        "\n",
        "          #if len(df_triaxial) > 0.5*ts:\n",
        "          print('Total len: ', len(df_triaxial))\n",
        "          print('Total null: ', df_triaxial[df_triaxial['Indoor Probability']==0.5]['Indoor Probability'].sum())\n",
        "          print('Total indoor: ', df_triaxial[df_triaxial['Indoor Probability']==1]['Indoor Probability'].sum())\n",
        "          print('Total null percentage: ', df_triaxial[df_triaxial['Indoor Probability']==0.5]['Indoor Probability'].sum()/len(df_triaxial)*100)\n",
        "          print('Total indoor percentage: ', df_triaxial[df_triaxial['Indoor Probability']==1]['Indoor Probability'].sum()/len(df_triaxial)*100)\n",
        "          print('Saving patient CSV file...')\n",
        "          df_triaxial = preprocess_subject(df_triaxial)\n",
        "          df_triaxial.to_csv(f'/content/drive/Shareddrives/Shared_Stefano_Vicenzo/Code/Data/df_{patient}.csv')\n",
        "          print('Saved!')\n",
        "          #else:\n",
        "          #  print('Too few samples for ID: ', patient)\n",
        "      except:\n",
        "          print(f'Corrupted patient file: {patient}')\n",
        "          continue\n",
        "      #print('--------------------------')\n",
        "  return full_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmPiyMVrUhng"
      },
      "outputs": [],
      "source": [
        "df = makeData(data_folder, data_format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz_uZawMxCwM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "61904c8fb5ddac050a66f3dc9523c0169d8b4e7e9ee2328227b1a6f5d66ad0af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
